# Assistive Navigation System - Project Overview

## üéØ Project Vision

A fully autonomous, wearable navigation aid for visually impaired users that combines:
- **Physical collision avoidance** via ultrasonic sensing
- **Environmental awareness** via AI vision
- **Adaptive ambient audio** that matches the surroundings
- **Zero user intervention** - completely autonomous operation

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   MAIN COORDINATOR                       ‚îÇ
‚îÇ                     (main.py)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                  ‚îÇ                  ‚îÇ
     ‚ñº                  ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇUltrasonic‚îÇ      ‚îÇ   Vision    ‚îÇ    ‚îÇ    Audio     ‚îÇ
‚îÇ  Module  ‚îÇ      ‚îÇ   Module    ‚îÇ    ‚îÇ   Engine     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                  ‚îÇ                   ‚îÇ
     ‚ñº                  ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇHC-SR04  ‚îÇ      ‚îÇUSB Camera   ‚îÇ    ‚îÇ Speakers/    ‚îÇ
‚îÇSensor   ‚îÇ      ‚îÇ+ TFLite     ‚îÇ    ‚îÇ Headphones   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Thread Model

The system runs **3 concurrent threads**:

1. **Ultrasonic Loop** (50 Hz)
   - Fast collision detection
   - Median filtering (5 samples)
   - Zone-based beeping
   - Hysteresis to prevent chatter

2. **Vision Loop** (2-5 Hz)
   - Scene classification (~2 FPS)
   - Object detection (~1.5 FPS)
   - Art detection heuristic
   - Stability voting (8-sample window)

3. **Audio Update Loop** (10 Hz)
   - Crossfade management
   - Volume ducking
   - TTS coordination

All threads communicate via shared `SystemState` object.

## üß† AI/ML Pipeline

### Scene Classification

```
Camera Frame (640√ó480 RGB)
    ‚Üì
Resize to 224√ó224
    ‚Üì
Normalize (0-1 or 0-255 depending on model)
    ‚Üì
TFLite Inference (MobileNetV2 INT8)
    ‚Üì
Softmax probabilities over 15-20 classes
    ‚Üì
Stability voting (8-frame window)
    ‚Üì
Map to soundscape folder
    ‚Üì
Trigger audio crossfade
```

**Performance**: ~8 FPS on Pi 4 (120ms per frame)

### Object Detection

```
Camera Frame (640√ó480 RGB)
    ‚Üì
Resize to 320√ó320
    ‚Üì
Normalize
    ‚Üì
TFLite Inference (YOLOv5n or SSD INT8)
    ‚Üì
[boxes, classes, scores]
    ‚Üì
Filter by confidence (>0.5)
    ‚Üì
Filter by priority (stairs, person, etc.)
    ‚Üì
Rate-limited TTS announcement (<1 per 5s)
```

**Performance**: ~5 FPS on Pi 4 (180ms per frame)

### Art Detection (Heuristic)

Simple contour detection for museum/gallery override:

```
Frame ‚Üí Grayscale ‚Üí Canny edges ‚Üí Find contours
    ‚Üì
Filter by area (12-60% of frame)
    ‚Üì
Check if 4-sided (rectangle-like)
    ‚Üì
If museum_gallery scene + frame detected ‚Üí classical music
```

**Performance**: <10ms (OpenCV contours)

## üîä Audio Architecture

### Three-Layer Mixing

```
Layer 1: Soundscape Bed (looping, 70% volume)
    ‚Üì
Layer 2: Collision Beeps (sine waves, 60% volume)
    ‚Üì
Layer 3: TTS Voice (espeak/say, 80% volume)
    ‚Üì
    Mix ‚Üí Duck when speaking/beeping ‚Üí Output
```

### Soundscape Selection Logic

```python
if scene == "museum_gallery" and art_detected:
    folder = "museum"  # Classical music
elif scene in scene_mapping:
    folder = scene_mapping[scene]
else:
    folder = "indoor"  # Default fallback

if folder != current_folder:
    crossfade(current_folder, folder, duration=3.5s)
```

### Beep Generation

Synthesized sine waves with frequency/rate scaling:

| Distance | Freq (Hz) | Interval (s) | Description |
|----------|-----------|--------------|-------------|
| > 2.0m | ‚Äî | ‚Äî | Silent |
| 1.2‚Äì2.0m | 440 | 0.65 | Low alert |
| 0.7‚Äì1.2m | 660 | 0.32 | Medium alert |
| 0.45‚Äì0.7m | 880 | 0.15 | High alert |
| < 0.45m | 1100 | continuous | DANGER + "Stop" |

## üìä Performance Characteristics

### CPU Usage

Typical load on Pi 4 (4GB):
- **Idle**: 5-10%
- **Running**: 40-60%
- **Peak**: 75% (during scene change + detection)

### Memory Usage

- **Base system**: ~200 MB
- **Python + deps**: ~150 MB
- **Models loaded**: ~50 MB
- **Audio buffers**: ~30 MB
- **Total**: ~450 MB (plenty of headroom on 4GB Pi)

### Power Consumption

- **Pi 4**: 3-5W
- **Camera**: 1-2W
- **Sensor**: 0.5W
- **Total**: ~5-7W average

**Battery life** with 10,000 mAh bank: 6-8 hours

### Latency

- **Collision warning**: <50ms (ultrasonic ‚Üí beep)
- **Scene change**: 4-6s (stability window + crossfade)
- **Object announcement**: 1-2s (detection ‚Üí TTS)

## üéõÔ∏è Configuration Philosophy

All tunable parameters in `config.yaml`:

- **Ultrasonic zones**: Distances, pitches, intervals
- **Vision timing**: Inference rates, stability window
- **Audio volumes**: All independent (soundscape, beeps, voice)
- **Scene mapping**: Flexible class ‚Üí folder mapping
- **Rate limits**: Prevent announcement spam

**No hardcoded magic numbers** - everything is configurable.

## üöÄ Deployment Scenarios

### Scenario 1: Daily Commute

- **Start**: Residential area ‚Üí calm suburban soundscape
- **Walk**: Detects "urban_street" ‚Üí city traffic soundscape
- **Enter**: "indoor_hall" ‚Üí quiet indoor ambient
- **Continuous**: Beeps warn of obstacles, people, stairs

### Scenario 2: Museum Visit

- **Detect**: "museum_gallery" scene
- **Heuristic**: Finds frame-like rectangles (paintings)
- **Override**: Switches to classical music
- **Experience**: Cultural ambience + navigation aid

### Scenario 3: Nature Walk

- **Detect**: "forest" or "park" scene
- **Audio**: Forest birds, rustling leaves
- **Quiet**: Fewer obstacles, less beeping
- **Immersive**: Nature sounds enhance experience

## üî¨ Technical Innovations

### 1. Hysteresis in Zone Detection

Prevents rapid beeping changes when hovering at boundary:

```python
if distance_decreasing:
    threshold = zone.min - hysteresis
else:
    threshold = zone.min + hysteresis
```

### 2. Scene Stability Voting

Requires majority agreement over N frames before switching:

```python
if scene_votes.count(new_scene) >= window_size // 2:
    trigger_scene_change(new_scene)
```

Prevents flickering between scenes.

### 3. Audio Ducking

Automatically lowers music when important:

```python
if speaking or beeping:
    soundscape_volume = ducked_volume  # 30%
else:
    soundscape_volume = normal_volume  # 70%
```

### 4. Rate-Limited Announcements

Prevents voice spam:

```python
if (current_time - last_speech) > rate_limit:
    speak(message)
    last_speech = current_time
```

### 5. Deduplication Memory

Doesn't repeat same object multiple times:

```python
announced_objects = set()
if object not in announced_objects:
    speak(f"{object} ahead")
    announced_objects.add(object)
    Timer(10.0, lambda: announced_objects.remove(object))
```

## üìà Future Enhancements

### Phase 2 (Short-term)

- [ ] **IMU sensor**: Improve stair detection with tilt angle
- [ ] **GPS module**: Location-aware soundscapes (home, work, etc.)
- [ ] **Depth camera**: Intel RealSense for better 3D awareness
- [ ] **Battery monitoring**: Warn when power low
- [ ] **Companion app**: Remote config via smartphone

### Phase 3 (Medium-term)

- [ ] **Multi-language**: TTS in multiple languages
- [ ] **Voice commands**: Optional "Hey Pi" activation
- [ ] **Cloud models**: Download new scenes/soundscapes OTA
- [ ] **Activity recognition**: Walking, running, sitting detection
- [ ] **Social features**: Share soundscape collections

### Phase 4 (Long-term)

- [ ] **Stereo audio**: Direction-aware spatial beeps
- [ ] **Haptic feedback**: Vibration motors for silent mode
- [ ] **Emergency alert**: Auto-call/SMS if fall detected
- [ ] **Smart home integration**: Voice control Alexa/Google
- [ ] **AR glasses**: Optional visual overlay (for low vision)

## üéì Educational Value

This project demonstrates:

- **Embedded systems**: Raspberry Pi GPIO, sensors
- **Computer vision**: Real-time inference, TFLite optimization
- **Audio processing**: Mixing, synthesis, crossfading
- **Multithreading**: Concurrent I/O without blocking
- **Software engineering**: Modular design, configuration management
- **Human-centered design**: Accessibility, autonomous operation

## ü§ù Contributing

Areas for contribution:

1. **Models**: Train better scene classifiers on specific domains
2. **Soundscapes**: Curate high-quality audio collections
3. **Hardware**: Design 3D-printable mounts and enclosures
4. **Testing**: Real-world user testing and feedback
5. **Documentation**: Tutorials, videos, translations
6. **Optimization**: Further performance improvements

## üìö Related Work

- **Microsoft Soundscape**: Spatial audio for navigation (discontinued)
- **BlindSquare**: GPS-based audio navigation app
- **BeSpecular**: Visual recognition via human volunteers
- **Orcam MyEye**: Wearable AI camera (commercial, $2000+)

**This project**: Open-source, affordable ($150-250), fully autonomous

## üìú License

MIT License - Free to use, modify, distribute

## üôè Credits

- **FSE100 Team**: Initial concept and implementation
- **TensorFlow**: ML framework and TFLite runtime
- **Raspberry Pi Foundation**: Affordable computing platform
- **OpenCV Community**: Computer vision tools
- **Pygame Contributors**: Audio mixing library

---

**Built with ‚ù§Ô∏è for accessibility and inclusion**

*"Technology should empower everyone, regardless of ability."*

